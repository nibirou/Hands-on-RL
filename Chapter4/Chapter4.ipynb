{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47a3474",
   "metadata": {},
   "source": [
    "# 第4章 动态规划算法\n",
    "\n",
    "## 4.1 简介\n",
    "动态规划（dynamic programming）是程序设计算法中非常重要的内容，能够高效地解决一些经典问题，例如背包问题和最短路径规划。动\n",
    "态规划的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会保存已解决的子\n",
    "问题的答案，在求解目标问题的过程中，需要这些子问题的答案时就可以直接利用，避免重复计算。本章介绍如何用动态规划的思想来求解马\n",
    "尔可夫决策过程中的最优策略。\n",
    "\n",
    "基于动态规划的强化学习算法主要有两种：一是策略迭代（policy iteration），二是价值迭代（value iteration）。其中，策略迭代由两部分组成：策略评估（policy evaluation）和策略提升（policy improvement）。\n",
    "具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值函数。\n",
    "\n",
    "不同于3.5 节介绍的蒙特卡洛方法和第5 章将要介绍的时序差分算法，基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。但是，现实中的白盒环境很少，这也是动态规划算法的局限之处，我们无法将其运用到很多实际场景中。另外，策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的。\n",
    "\n",
    "## 4.2 悬崖漫步环境\n",
    "本节使用策略迭代和价值迭代来求解悬崖漫步（Cliff Walking）这个环境中的最优策略。接下来先简单介绍一下该环境。\n",
    "悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。如图4-1 所示，有一个4×12\n",
    "的网格世界，每一个网格表示一个状态。智能体的起点是左下角的状态，目标是右下角的状态，智能体在每一个状态都可以采取4 种动作：上、下、左、右。如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。智能体每走一步的奖励是−1，掉入悬崖的奖励是−100。\n",
    "\n",
    "## 4.3 策略迭代算法\n",
    "策略迭代是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。本节分别对这两个过程进行详细介绍。\n",
    "\n",
    "## 4.3.1 策略评估\n",
    "策略评估这一过程用来计算一个策略的状态价值函数。根据贝尔曼期望方程\n",
    "可知，当知道奖励函数和状态转移函数时，可以根据下一个状态的价值来计算当前状态的价值。因此，根据动态规划的思想，\n",
    "可以把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题。\n",
    "\n",
    "## 4.3.2 策略提升\n",
    "使用策略评估计算得到当前策略的状态价值函数之后，我们可以据此来改进该策略。假设此时对于策略Π，我们已经知道了其价值，也就是知道了在策略Π下从每一个状态s出发最终得到的期望回报。\n",
    "这是，我们要改变策略"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
