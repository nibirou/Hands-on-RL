{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cadfae46",
   "metadata": {},
   "source": [
    "# 3.1 简介\n",
    "马尔可夫决策过程（Markov decision process，MDP）是强化学习的重要概念。\n",
    "与多臂老虎机问题不同，马尔可夫决策过程包含状态信息以及状态之间的转移机制。如果要用强化学习去解决一个实际问题，\n",
    "第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。本章\n",
    "将从马尔可夫过程出发，一步一步地进行介绍，最后引出马尔可夫决策过程。\n",
    "\n",
    "# 3.2 马尔可夫过程\n",
    "## 3.2.1 随机过程\n",
    "随机过程（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，\n",
    "而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。\n",
    "在随机过程中，随机现象在某时刻t 的取值是一个向量随机变量，用t S 表示，所有可能的状态组成状\n",
    "态集合。随机现象便是状态的变化过程。\n",
    "\n",
    "## 3.2.2 马尔可夫性质\n",
    "当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质（Markov property）。也就是说当前状态是未来状态的充分统计量，即下一时刻状态只取决于当前状态，而不会收到过去状态的影响。\n",
    "\n",
    "需要明确的是，具有马尔可夫并不代表这个随机过程就和历史完全没有关系，因为虽然t+1时刻的状态只与t时刻的状态有关，但是t时刻的状态其实包含了t-1时刻状态的信息。通过这种链式关系，历史的信息被传递到了现在。\n",
    "\n",
    "马尔可夫性质可以大大简化运算，因为只要当前状态可知，所有的历史信息就都不再需要了，利用当前状态的信息就可以决定未来。\n",
    "\n",
    "## 3.2.3 马尔可夫过程\n",
    "马尔可夫过程（Markov process）是指具有马尔可夫性质的随机过程，也被称为马尔可夫链（Markov Chain）。我们通常用元组（S，P）描述一个\n",
    "马尔可夫过程，其中S是有限数量的状态集合，P是状态转移矩阵，定义了所有状态对之间的转移概率。从某个状态出发，到达其他状态的概率和必须为1，即状态转移矩阵P的每一行和为1.\n",
    "\n",
    "给定一个马尔可夫过程，我们就可以从某个状态出发，根据他的状态转移矩阵生成一个状态序列（episode），这个步骤也被叫做采样（sampling）。\n",
    "\n",
    "# 3.3 马尔可夫奖励过程\n",
    "在马尔可夫过程的基础上加入奖励函数r和折扣因子γ，就可以得到马尔可夫奖励过程，（Markov reward process，MRP）。\n",
    "\n",
    "r 奖励函数：某个状态 s 的奖励 r(s)指转移到该状态时可以获得的奖励的期望。\n",
    "γ 折扣因子（discount factor， 取值范围为[0, 1]）：引入折扣因子，是因为远期利益具有一定的不确定性，有时我们更希望能够\n",
    "尽快获得一些奖励，所以需要对远期利益打一些折扣。接近1的γ更关注长期的累计奖励，接近0的更考虑从短期奖励\n",
    "\n",
    "## 3.3.1 回报\n",
    "在一个马尔科夫奖励过程中，从第t时刻的状态St开始，直到终止状态时，所有奖励的衰减之和称为回报。（Return）\n",
    "\n",
    "## 3.3.2 价值函数\n",
    "在马尔可夫奖励过程中，一个状态的期望回报被称为这个状态的价值（value）。所有状态的价值就组成了价值函数（value function），价值函数\n",
    "的输入为某个状态，输出为这个状态的价值。\n",
    "\n",
    "# 3.4 马尔可夫决策过程\n",
    "3.2节和3.3节讨论到的马尔可夫过程和马尔科夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变\n",
    "这个随机过程，就有了马尔可夫决策过程（Markov decision process，MDP）。\n",
    "\n",
    "我们将这个来自外界的刺激成为智能体（Agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程MDP。\n",
    "\n",
    "此时奖励函数同时取决于状态s和动作a，在奖励函数只取决于状态s时，退化为MRP中的奖励函数r(s)\n",
    "\n",
    "MDP和MRP非常相像，主要区别是MDP中的状态转移函数和奖励函数都比MRP中做了作为自变量的动作a。\n",
    "在 MDP 的定义中，我们不再使用类似 MRP 定义中的状态转移矩阵，而是直接使用状态转移函数。这样做一是因为此时的状态转移与动作也\n",
    "有关，变成了一个三维数组，而不再是一个矩阵（二维数组）；二是因为状态转移函数更具有一般意义，例如，如果状态集合不是有限的，就无法用数组表示，但仍然可以用状态转移函数表示。我们在之后的学习中会遇到连续状态的 MDP 环境，那时状态集合都不是有限的。现在我们主要关注离散状态的 MDP 环境，此时状态集合是有限的\n",
    "\n",
    "不同于马尔可夫奖励过程，在马尔可夫决策过程中，通常存在一个智能体来执行动作。例如，一艘小船在大海中随着水流自由飘荡的过程就是一个马尔可夫奖励过程，它如果凭借运气漂到了一个目的地，就能获得比较大的奖励；如果有水手在控制着这条船往哪个方向前进，就可以主动选择前往目的地来获得比较大的奖励。马尔可夫决策过程是一个与时间相关的不断进行的过程，在智能体和环境 MDP 之间存在一个不断交互的过程\n",
    "\n",
    "# 3.5 蒙特卡洛方法\n",
    "蒙特卡洛方法（Monte Carlo method）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用蒙特卡洛方法时，我们通常使用重复随机抽样，\n",
    "然后运用概率统计方法来从抽样结果中归纳出我们想得到的目标的数值估计。\n",
    "\n",
    "如何用蒙特卡洛方法来估计一个策略，在一个马尔可夫决策过程中的状态价值。回忆一下，一个状态的价值是它的期望回报，那么一个很直观的想法就是\n",
    "用策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了\n",
    "\n",
    "# 3.6 占用度量\n",
    "3.4节提到了不同策略的价值函数不同，因为对于同一个MDP，不同策略会访问到的状态的概率分布不同。\n",
    "不同策略会使得智能体访问到不同概率分布的状态，这会影响到策略的价值函数。\n",
    "\n",
    "先定义MDP的初始状态分布v（MDP的初始状态并不一定是一个定值，也有可能是一个分布），策略Π使得智能体在t时刻状态为s的概率P，目前就是要求\n",
    "一个策略的状态访问分布，以及 折扣状态访问分布 下的递推公式，进而定义占用度量\n",
    "\n",
    "# 3.7 最优策略\n",
    "强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。\n",
    "在有限状态和动作集合的MDP中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是最优策略（optimal policy）。最优策略可能有很多个。\n",
    "最优策略都有相同的状态价值函数，称为最优状态价值函数，某个状态s的状态价值函数表示为从状态s出发遵循策略Π能获得的期望回报\n",
    "\n",
    "同理，定义最优动作价值函数，不同于MRP，在MDP中，由于动作的存在，额外定义动作价值函数，来表示遵循策略Π时，对当前状态s执行动作a得到的期望回报，\n",
    "\n",
    "状态价值函数和动作价值函数之间的关系为：在使用策略Π时，状态 s 的价值等于在该状\n",
    "态下基于策略Π采取所有动作的概率与相应的价值相乘再求和的结果："
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
